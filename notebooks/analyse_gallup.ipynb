{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gallup Analyse Notebook\n",
    "\n",
    "Author: Jade Bullock\n",
    "\n",
    "Date: 06/04/2024\n",
    "\n",
    "This Jupyter Notebook performs exploratory data analysis and modeling on the merged Gallup Global dataset, with the aim of identifying which factors best predict national happiness (as measured by the\n",
    "World Happiness Report's \"Happiness Index\").\n",
    "\n",
    "## Main Steps:\n",
    "-----------\n",
    "1. Load the cleaned and merged Gallup dataset.\n",
    "2. Visualize correlations via heatmaps and bar charts.\n",
    "3. Explore individual variable relationships using scatter plots.\n",
    "4. Fit and compare two predictive models:\n",
    "    - Linear Regression\n",
    "    - Random Forest Regressor\n",
    "5. Analyze and interpret feature importance from both models.\n",
    "6. Evaluate model accuracy using R² and RMSE.\n",
    "7. Highlight key insights, including expected and surprising findings (e.g. smiled_yes behavior).\n",
    "\n",
    "## Outputs:\n",
    "--------\n",
    "- Heatmap and bar plots of feature correlations\n",
    "- Scatter plots of happiness vs top features\n",
    "- Ranked feature importances (coefficients and tree-based)\n",
    "- Model accuracy summary (R², RMSE)\n",
    "- A written summary of predictive insights\n",
    "\n",
    "## Note:\n",
    "-----\n",
    "The analysis excludes *_no variables to avoid multicollinearity and focuses only\n",
    "on *_yes indicators or scaled scores (e.g. law and order)."
   ],
   "id": "143a1d968792f39a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ],
   "id": "ee027d9b00643d22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Load the cleaned and merged Gallup dataset.",
   "id": "45afc9742966cf70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load Gallup merged dataset\n",
    "gallup_df = pd.read_csv(\"../data/clean/gallup_merge.csv\")"
   ],
   "id": "bfde7b3f0faac4e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Visualize correlations via heatmaps and bar charts.",
   "id": "9ee25c862ea45659"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Select only numeric columns for correlation analysis\n",
    "numeric_data = gallup_df.select_dtypes(include='number')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = numeric_data.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Generate a heatmap using seaborn\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,          # show correlation coefficients\n",
    "    fmt=\".2f\",           # format to 2 decimal places\n",
    "    cmap=\"coolwarm\",     # color map\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8}\n",
    ")\n",
    "\n",
    "# Add title and adjust layout\n",
    "plt.title(\"Correlation Heatmap of Gallup Indicators (Including Happiness Index)\", fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(\"visuals/gallup_correlation_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "print(\"Heatmap saved to: visuals/gallup_correlation_heatmap.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "853e02895557cd4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract correlations with 'Happiness Index' only\n",
    "target = \"Happiness Index\"\n",
    "correlation_df = corr_matrix[[target]].drop(index=target).reset_index()\n",
    "correlation_df.columns = [\"Indicator\", \"Correlation with Happiness\"]\n",
    "correlation_df = correlation_df.sort_values(\"Correlation with Happiness\", ascending=False)\n",
    "\n",
    "\n",
    "# Create bar chart of correlation results\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=correlation_df,\n",
    "    x=\"Correlation with Happiness\",\n",
    "    y=\"Indicator\", hue=\"Indicator\",\n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "\n",
    "plt.title(\"Correlation of Indicators with Happiness Index\")\n",
    "plt.xlabel(\"Pearson Correlation\")\n",
    "plt.ylabel(\"Indicator\")\n",
    "plt.axvline(0, color='gray', linestyle='--')  # zero line for reference\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the chart\n",
    "plt.savefig(\"visuals/happiness_correlation_barplot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\" Bar chart saved to: visuals/happiness_correlation_barplot.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "id": "669f3af7f1c0a7f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Explore individual variable relationships against Happiness Index using scatter plots.",
   "id": "c307267afe2de505"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define key variables to plot against the Happiness Index\n",
    "key_variables = [\n",
    "    \"sadness_yes\",\n",
    "    \"enjoyment_yes\",\n",
    "    \"respect_yes\",\n",
    "    \"pain_yes\",\n",
    "    \"PERCENTAGE_Safety\",\n",
    "    \"SCORE_law_order\"\n",
    "]\n",
    "\n",
    "# Create a 2x3 grid of scatter plots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(key_variables):\n",
    "    sns.scatterplot(\n",
    "        data=gallup_df,\n",
    "        x=var,\n",
    "        y=\"Happiness Index\",\n",
    "        ax=axes[i],\n",
    "        edgecolor=\"w\"\n",
    "    )\n",
    "    axes[i].set_title(f\"Happiness Index vs. {var}\", fontsize=12)\n",
    "    axes[i].set_xlabel(var)\n",
    "    axes[i].set_ylabel(\"Happiness Index\")\n",
    "\n",
    "# Layout adjustment\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"visuals/happiness_scatter_plots1.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\" Scatter plots saved to: visuals/happiness_scatter_plots1.png\")\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ],
   "id": "7b2885a2a47ac7d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define key variables to plot against the Happiness Index\n",
    "key_variables = [\n",
    "    \"anger_yes\",\n",
    "    \"learned_yes\",\n",
    "    \"worry_yes\",\n",
    "    \"well-rested_yes\",\n",
    "    \"smiled_yes\",\n",
    "    \"stress_yes\"\n",
    "]\n",
    "\n",
    "# Create a 2x3 grid of scatter plots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, var in enumerate(key_variables):\n",
    "    sns.scatterplot(\n",
    "        data=gallup_df,\n",
    "        x=var,\n",
    "        y=\"Happiness Index\",\n",
    "        ax=axes[i],\n",
    "        edgecolor=\"w\"\n",
    "    )\n",
    "    axes[i].set_title(f\"Happiness Index vs. {var}\", fontsize=12)\n",
    "    axes[i].set_xlabel(var)\n",
    "    axes[i].set_ylabel(\"Happiness Index\")\n",
    "\n",
    "# Layout adjustment\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"visuals/happiness_scatter_plots2.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\" Scatter plots saved to: visuals/happiness_scatter_plots2.png\")\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ],
   "id": "4822ce5e56914139"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Fit and compare two predictive models",
   "id": "53bc4a0771239ada"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Use regression to understand which Gallup indicators are most predictive of a country's Happiness Index.\n",
    "Since the _yes and _no pairs are strongly negatively correlated, as expected (e.g. enjoyment_yes vs enjoyment_no, use regression on only one value.  This is to prevent distorting results (multicollinearity)."
   ],
   "id": "5a3b1d16996c04c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Drop non-numeric columns (like Country) since they can't be used in regression.\n",
    "- Drop rows with missing values to keep the analysis simple and clean.\n",
    "- Normalise the data or use StandardScaler to put all features on the same scale.  (mean = 0 std = 1).\n",
    "- Train a LinearRegression model using the cleaned data.\n",
    "- Use model to assign coefficients to each feature based on how much it contributes to predicting the Happiness Index.\n",
    "- Interpret the Coefficients for linear model\n",
    "    - Positive coefficients = variables that increase happiness;\n",
    "    - Negative coefficients = variables that decrease happiness;\n",
    "    - Higher absolute value = more predictive power.\n"
   ],
   "id": "5b7e2b02533b7fa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean and Prepare the Data\n",
    "\n",
    "df_filtered = (\n",
    "    gallup_df\n",
    "    .dropna()  # Remove rows with missing values\n",
    "    .drop(columns=[col for col in gallup_df.columns if col.endswith(\"_no\")])  # Drop '_no' columns\n",
    ")\n",
    "X = df_filtered.drop(columns=[\"Country\", \"Happiness Index\"])  # remove the predictor and the non-numeric values\n",
    "y = df_filtered[\"Happiness Index\"]  # Assign as target predictor\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ],
   "id": "a8ce9deb14fdfb7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Linear Model",
   "id": "c95c623ea831ac91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fit the Linear model\n",
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_scaled, y)\n",
    "\n",
    "# Extract and rank feature importance\n",
    "coefficients = pd.Series(lin_model.coef_, index=X.columns)\n",
    "coeff_df = coefficients.sort_values(ascending=False).reset_index()\n",
    "coeff_df.columns = [\"Feature\", \"Predictive Strength (Coefficient)\"]\n",
    "print(coeff_df)\n",
    "\n",
    "#  Visualize the top predictors\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=coeff_df, x=\"Predictive Strength (Coefficient)\", y=\"Feature\", hue=\"Feature\", palette=\"coolwarm\")\n",
    "plt.title(\"Most Predictive Features of Happiness (Linear Regression)\")\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"visuals/Linear_reg_predictive_features.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\" Plot saved to: visuals/Linear_reg_predictive_features.png\")\n",
    "plt.show()"
   ],
   "id": "9a9e6cf827b66582"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Enjoyment_yes: +0.48 Most powerful positive predictor. More enjoyment → more happiness.\n",
    "smiled_yes: -0.47 Unexpected!  Does not match the scatter plot relationship - has shown a strong negative relationship.  Indicating an issue with the regression.\n",
    "Learned_yes: +0.39 Learning something interesting is highly linked to happiness.\n",
    "Sadness_yes: -0.36 Strongly associate with unhappiness\n",
    "Score_law_order:Trust in law and order boosts national happiness.\n"
   ],
   "id": "2240452a221bf731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Random forest (RandomForestRegressor)\n",
   "id": "58b369ce7b347f2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Fit Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_scaled, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "rf_importance_df = pd.Series(importances, index=X.columns).sort_values(ascending=False).reset_index()\n",
    "rf_importance_df.columns = [\"Feature\", \"RF_Importance\"]\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nTop Predictors (Random Forest):\")\n",
    "print(rf_importance_df.head(10))\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=rf_importance_df.head(10), x=\"RF_Importance\", y=\"Feature\", hue=\"Feature\", palette=\"viridis\")\n",
    "plt.title(\"Top 10 Predictors of Happiness (Random Forest)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"visuals/random_forest_predictive_features.png\", dpi=300, bbox_inches=\"tight\")\n",
    "print(\" Scatter plots saved to: visuals/random_forest_predictive_features.png\")\n",
    "plt.show()"
   ],
   "id": "cce7eb4a1c631387"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- pain_yes: 36.5% Most important predictor. Pain dramatically lowers happiness.\n",
    "- respect_yes: 15.6% Feeling respected plays a huge role in perceived well-being.\n",
    "- SCORE_law_order: 10.0% Trust in institutions and rule of law strongly tied to happiness.\n",
    "- enjoyment_yes: 7.6% Still a strong positive predictor — validates linear model.\n",
    "- learned_yes: 6.7% Learning contributes meaningfully to happiness.\n",
    "- smiled_yes: 4.9% Less dominant than in linear model — but still relevant.\n",
    "- sadness_yes: 4.3% Directly tied to lower happiness.\n",
    "- PERCENTAGE_Safety: 3.9% Confirms that personal safety is part of happiness.\n",
    "- well-rested_yes: 3.2% Sleep and rest affect well-being.\n",
    "- stress_yes: 2.8% Stress still matters, though not dominant.\n"
   ],
   "id": "639495ab1387239c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Analyze and interpret feature importance from both models.\n",
    "Compare Linear vs. Random Forest Rankings"
   ],
   "id": "739980cbae4ec11f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Merge and compare\n",
    "comparison_df = pd.merge(coeff_df, rf_importance_df, on=\"Feature\")\n",
    "comparison_df = comparison_df.sort_values(\"RF_Importance\", ascending=False)\n",
    "print(comparison_df)"
   ],
   "id": "2d04065712dd3fbc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate univariate Pearson correlation for each feature with Happiness Index\n",
    "correlations = {\n",
    "    feature: df_filtered[feature].corr(df_filtered[\"Happiness Index\"])\n",
    "    for feature in comparison_df[\"Feature\"]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "correlation_df = pd.DataFrame.from_dict(correlations, orient=\"index\", columns=[\"Pearson Correlation\"])\n",
    "correlation_df.reset_index(inplace=True)\n",
    "correlation_df.rename(columns={\"index\": \"Feature\"}, inplace=True)\n",
    "\n",
    "# Drop existing Pearson Correlation columns before merging (just in case)\n",
    "comparison_df = comparison_df.drop(\n",
    "    columns=[col for col in comparison_df.columns if \"Pearson Correlation\" in col],\n",
    "    errors=\"ignore\"\n",
    ")\n",
    "\n",
    "# Merge correlation back in\n",
    "comparison_df = comparison_df.merge(correlation_df, on=\"Feature\")\n",
    "\n",
    "# Sort again by Random Forest importance\n",
    "comparison_df = comparison_df.sort_values(\"RF_Importance\", ascending=False)\n",
    "\n",
    "# Show results\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n"
   ],
   "id": "b8da0fb8dc89a8be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. Evaluate model accuracy using R² and RMSE",
   "id": "6e98dac0f7de5168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cross-validate R^2 scores\n",
    "lin_r2 = cross_val_score(lin_model, X_scaled, y, cv=5, scoring='r2')\n",
    "rf_r2 = cross_val_score(rf_model, X_scaled, y, cv=5, scoring='r2')\n",
    "\n",
    "# Train/Test RMSE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear model\n",
    "lin_model.fit(X_train, y_train)\n",
    "y_pred_lin = lin_model.predict(X_test)\n",
    "lin_rmse = np.sqrt(mean_squared_error(y_test, y_pred_lin))\n",
    "\n",
    "# Random Forest model\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "# Correlation of individual predictor\n",
    "smile_corr = df_filtered[\"smiled_yes\"].corr(df_filtered[\"Happiness Index\"])\n",
    "\n",
    "# Generate results\n",
    "print(\"\\nModel Accuracy Comparison\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Linear Regression R² (mean CV): {lin_r2.mean():.3f}\")\n",
    "print(f\"Random Forest R² (mean CV):    {rf_r2.mean():.3f}\")\n",
    "print(f\"Linear Regression RMSE:        {lin_rmse:.3f}\")\n",
    "print(f\"Random Forest RMSE:            {rf_rmse:.3f}\")\n"
   ],
   "id": "1dba60facc27bc25"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Summary and key insights\n",
    "\n",
    "Top Predictive Indicators (Across Linear & Random Forest Models)\n",
    "- Enjoyment (enjoyment_yes): The strongest positive predictor of happiness. Consistently ranked high in both regression and tree-based models.\n",
    "- Respect (respect_yes): Feeling respected is highly predictive of higher happiness. Social dignity matters.\n",
    "- Pain (pain_yes): Negatively correlated and the most important feature in Random Forest — highlighting how physical/emotional suffering impacts well-being.\n",
    "- Learning (learned_yes): Associated with increased happiness, indicating the role of engagement and intellectual stimulation.\n",
    "- Law & Order (SCORE_law_order): Trust in legal systems and safety correlates strongly with well-being.\n",
    "\n",
    "Unexpected Findings\n",
    "- Smiled_yes had a positive correlation with happiness (as expected), but a negative coefficient in the linear model. This likely indicates:\n",
    "    - Multicollinearity (strong correlation with enjoyment_yes or other positive emotions),\n",
    "    - A suppression effect, where its true impact is masked when modeled jointly with correlated variables."
   ],
   "id": "e37e60d3d17d579c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
